{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "appropriate-lighting",
   "metadata": {},
   "source": [
    "# Optimisation\n",
    "\n",
    "This part is to look at the resources needed to apply some methods to obtain the final results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-preference",
   "metadata": {},
   "source": [
    "We first loaded the functions and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-zoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"schedule_lib.jl\")\n",
    "using BenchmarkTools\n",
    "\n",
    "students_filename = \"examparams.xlsx\"\n",
    "prof_filename = \"professors.xlsx\"\n",
    "function get_data()\n",
    "    professors = import_prof(prof_filename)\n",
    "    s = import_excel(students_filename,professors)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-guarantee",
   "metadata": {},
   "source": [
    "## Simple backtracking\n",
    "\n",
    "We first use the backtracking with the constraints only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "arbitrary-extra",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  13.79 MiB\n",
       "  allocs estimate:  140716\n",
       "  --------------\n",
       "  minimum time:     7.617 ms (0.00% GC)\n",
       "  median time:      8.804 ms (0.00% GC)\n",
       "  mean time:        10.162 ms (13.03% GC)\n",
       "  maximum time:     21.703 ms (30.32% GC)\n",
       "  --------------\n",
       "  samples:          492\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = get_data()\n",
    "@benchmark backtracking_search($s,inference=$apply_prep!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-island",
   "metadata": {},
   "source": [
    "## Backtracking with MCV\n",
    "\n",
    "We now apply the backtracking with MVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "radical-cancer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  61.43 MiB\n",
       "  allocs estimate:  617212\n",
       "  --------------\n",
       "  minimum time:     39.400 ms (9.62% GC)\n",
       "  median time:      41.631 ms (10.11% GC)\n",
       "  mean time:        42.601 ms (13.28% GC)\n",
       "  maximum time:     54.092 ms (10.12% GC)\n",
       "  --------------\n",
       "  samples:          118\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = get_data()\n",
    "@benchmark backtracking_search($s,inference=$apply_prep!,select_unassigned_variable=$MCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-fireplace",
   "metadata": {},
   "source": [
    "## Backtracking with MCV and LCV\n",
    "\n",
    "We now apply the backtracking with MVC and LCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "turkish-tragedy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  125.43 MiB\n",
       "  allocs estimate:  1272135\n",
       "  --------------\n",
       "  minimum time:     78.010 ms (10.09% GC)\n",
       "  median time:      85.447 ms (15.51% GC)\n",
       "  mean time:        89.387 ms (14.71% GC)\n",
       "  maximum time:     116.495 ms (14.65% GC)\n",
       "  --------------\n",
       "  samples:          56\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = get_data()\n",
    "@benchmark backtracking_search($s,inference=$apply_prep!,\n",
    "    select_unassigned_variable=$MCV,order_domain_values=$LCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-marble",
   "metadata": {},
   "source": [
    "## Backtracking with arc consistency\n",
    "\n",
    "We now apply the backtracking with arc consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "responsible-complaint",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  1.98 GiB\n",
       "  allocs estimate:  20531557\n",
       "  --------------\n",
       "  minimum time:     1.442 s (13.81% GC)\n",
       "  median time:      1.462 s (13.62% GC)\n",
       "  mean time:        1.476 s (13.62% GC)\n",
       "  maximum time:     1.537 s (13.57% GC)\n",
       "  --------------\n",
       "  samples:          4\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = get_data()\n",
    "@benchmark backtracking_search($s,inference=$apply_arc_consistency!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-philadelphia",
   "metadata": {},
   "source": [
    "## Backtracking with arc consistency and MVC\n",
    "\n",
    "We now apply the backtracking with arc consistency and MVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sustained-trading",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  2.56 GiB\n",
       "  allocs estimate:  26743083\n",
       "  --------------\n",
       "  minimum time:     1.687 s (13.70% GC)\n",
       "  median time:      1.710 s (14.10% GC)\n",
       "  mean time:        1.717 s (14.00% GC)\n",
       "  maximum time:     1.755 s (14.17% GC)\n",
       "  --------------\n",
       "  samples:          3\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = get_data()\n",
    "@benchmark backtracking_search($s,inference=$apply_arc_consistency!,\n",
    "    select_unassigned_variable=$MCV)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.2",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
